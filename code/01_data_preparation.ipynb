{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63c63f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://www.aeon-toolkit.org/en/stable/examples/datasets/load_data_from_web.html\n",
    "import os\n",
    "from aeon.datasets import load_classification\n",
    "from ksfdtw.utils import nearest_neighbor_interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2b16e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_concatenate(ts, seg_lengths):\n",
    "    \"\"\"\n",
    "    Concatenate a time series ts according to seg_lengths.\n",
    "    Each segment is shrunk to the specified length using nearest-neighbor interpolation.\n",
    "    \"\"\"\n",
    "    P = len(seg_lengths)\n",
    "    segments = []\n",
    "    cutting_points = []\n",
    "    start = 0\n",
    "    for p in range(P):\n",
    "        if seg_lengths[p] <= 0:\n",
    "            raise ValueError(\"Segment length must be positive.\")\n",
    "        seg_rescaled = nearest_neighbor_interpolation(ts, seg_lengths[p])\n",
    "        end = start + seg_lengths[p]\n",
    "        segments.append(seg_rescaled)\n",
    "        cutting_points.append((start, end))\n",
    "        start = end\n",
    "    concatenated = np.concatenate(segments)\n",
    "    return concatenated, cutting_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cf64e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset_uniform(X, P=3):\n",
    "    \"\"\"\n",
    "    Apply segment_concatenate transformation to every time series in X.\n",
    "    \"\"\"\n",
    "    L = len(X[0][0])  # The second index refers to the channel index. Univariate => 0\n",
    "    seg_lengths = [L // P + (1 if i < L % P else 0) for i in range(P)]\n",
    "\n",
    "    transformed_segs = []\n",
    "    cutting_points_list = []\n",
    "    for ts in X:\n",
    "        seg, cutting_points = segment_concatenate(ts[0], seg_lengths)\n",
    "        transformed_segs.append(seg)\n",
    "        cutting_points_list.append(cutting_points)\n",
    "    return np.array(transformed_segs), np.array(cutting_points_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1227729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_segment_lengths(L, P, l_min, l_max):\n",
    "    \"\"\"\n",
    "    Generate random segment lengths that sum to L.\n",
    "    Each segment length is between l_min and l_max.\n",
    "    \"\"\"\n",
    "    if P * l_min > L or P * l_max < L:\n",
    "        raise ValueError(\"Cannot generate segment lengths with given constraints.\")\n",
    "\n",
    "    while True:\n",
    "        seg_lengths = [random.randint(l_min, l_max) for _ in range(P)]\n",
    "        total_length = sum(seg_lengths)\n",
    "        if total_length == L:\n",
    "            return seg_lengths\n",
    "        # Adjust lengths to match L\n",
    "        diff = L - total_length\n",
    "        for i in range(P):\n",
    "            if diff == 0:\n",
    "                break\n",
    "            if diff > 0 and seg_lengths[i] < l_max:\n",
    "                seg_lengths[i] += 1\n",
    "                diff -= 1\n",
    "            elif diff < 0 and seg_lengths[i] > l_min:\n",
    "                seg_lengths[i] -= 1\n",
    "                diff += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bee0a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset_random(X, P=3, l=1.5):\n",
    "    \"\"\"\n",
    "    Apply segment_concatenate transformation with random segment lengths to every time series in X.\n",
    "    \"\"\"\n",
    "    m = len(X[0][0])\n",
    "    L_avg = m / P\n",
    "    l_root = math.sqrt(l)\n",
    "    L_min = max(1, int(L_avg / l_root))\n",
    "    L_max = min(int(L_avg * l_root), m)\n",
    "\n",
    "    transformed_segs = []\n",
    "    cutting_points_list = []\n",
    "    for ts in X:\n",
    "        seg_lengths = generate_random_segment_lengths(len(ts[0]), P, L_min, L_max)\n",
    "        seg, cutting_points = segment_concatenate(ts[0], seg_lengths)\n",
    "        transformed_segs.append(seg)\n",
    "        cutting_points_list.append(cutting_points)\n",
    "    return np.array(transformed_segs), np.array(cutting_points_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "076c7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "# length <= 200: SyntheticControl, ECG200, CBF, GunPoint, Adiac\n",
    "# length > 200: Coffee, Beef, Lighting7, FaceFour, OliveOil <- temporary\n",
    "dataset_name = \"Adiac\"\n",
    "# https://www.aeon-toolkit.org/en/stable/api_reference/auto_generated/aeon.datasets.load_classification.html\n",
    "X_train, y_train = load_classification(\n",
    "    name=dataset_name, split=\"train\", extract_path=\"../data/\"\n",
    ")\n",
    "X_test, y_test = load_classification(\n",
    "    name=dataset_name, split=\"test\", extract_path=\"../data/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a246e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 2  # number of segments\n",
    "l = 1.5  # scaling factor\n",
    "\n",
    "X_train_trans_uniform = transform_dataset_uniform(X_train, P)\n",
    "X_test_trans_uniform = transform_dataset_uniform(X_test, P)\n",
    "\n",
    "X_train_trans_random = transform_dataset_random(X_train, P, l=l)\n",
    "X_test_trans_random = transform_dataset_random(X_test, P, l=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23e54ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look at the time series after transformation\n",
    "\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.plot(X_train_trans_random[0][4])\n",
    "# plt.title(\"Random Time Series\")\n",
    "# plt.xlabel(\"Time\")\n",
    "# plt.ylabel(\"Value\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# X_train_trans_random[1][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a039ed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform dataset saved to ../data_processed/Adiac_P2_uniform.npz\n",
      "Random dataset saved to ../data_processed/Adiac_P2_l1.5_random.npz\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"../data_processed/\"\n",
    "file_name = f\"{dataset_name}_P{P}_uniform.npz\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.savez_compressed(\n",
    "    os.path.join(save_dir, file_name),\n",
    "    X_train_trans_uniform_concatenated=X_train_trans_uniform[0],\n",
    "    X_train_trans_uniform_cutting_points=X_train_trans_uniform[1],\n",
    "    y_train=y_train,\n",
    "    X_test_trans_uniform_concatenated=X_test_trans_uniform[0],\n",
    "    X_test_cutting_points=X_test_trans_uniform[1],\n",
    "    y_test=y_test,\n",
    ")\n",
    "print(f\"Uniform dataset saved to {os.path.join(save_dir, file_name)}\")\n",
    "\n",
    "file_name = f\"{dataset_name}_P{P}_l{l}_random.npz\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.savez_compressed(\n",
    "    os.path.join(save_dir, file_name),\n",
    "    X_train_trans_random_concatenated=X_train_trans_random[0],\n",
    "    X_train_trans_random_cutting_points=X_train_trans_random[1],\n",
    "    y_train=y_train,\n",
    "    X_test_trans_random_concatenated=X_test_trans_random[0],\n",
    "    X_test_trans_random_cutting_points=X_test_trans_random[1],\n",
    "    y_test=y_test,\n",
    ")\n",
    "print(f\"Random dataset saved to {os.path.join(save_dir, file_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ca11a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was last run end-to-end on: 2025-11-09 17:11:38.983454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "print(f\"This notebook was last run end-to-end on: {datetime.datetime.now()}\\n\")\n",
    "###\n",
    "###\n",
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ksfdtw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
